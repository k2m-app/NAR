import time
import json
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================

KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ç­‰ã§ set_race_params ãŒå‘¼ã°ã‚Œã‚‹ã¨æ›¸ãæ›ã‚ã‚‹ï¼‰
YEAR = "2025"
PLACE_CODE = "11"
MONTH = "12"
DAY = "16"


def set_race_params(year, place_code, month, day):
    """app.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å·®ã—æ›¿ãˆã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, PLACE_CODE, MONTH, DAY
    YEAR = str(year)
    PLACE_CODE = str(place_code).zfill(2)
    MONTH = str(month).zfill(2)
    DAY = str(day).zfill(2)


# ==================================================
# Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def save_history(
    year: str,
    place_code: str,
    place_name: str,
    month: str,
    day: str,
    race_num_str: str,
    race_id: str,
    ai_answer: str,
) -> None:
    """history ãƒ†ãƒ¼ãƒ–ãƒ«ã« 1 ãƒ¬ãƒ¼ã‚¹åˆ†ã®äºˆæƒ³ã‚’ä¿å­˜ã™ã‚‹ã€‚"""
    supabase = get_supabase_client()
    if supabase is None:
        return

    # åœ°æ–¹ç«¶é¦¬ã«åˆã‚ã›ã¦ã‚«ãƒ©ãƒ å†…å®¹ã¯é©å®œèª­ã¿æ›¿ãˆã¦ä¿å­˜
    # (æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ã‚’å¤‰ãˆãªã„ãŸã‚ã€kaiã‚„dayã«ä¾¿å®œçš„ã«å€¤ã‚’å…¥ã‚Œã‚‹)
    data = {
        "year": str(year),
        "kai": "",          # åœ°æ–¹ã§ã¯ä¸ä½¿ç”¨ã®ãŸã‚ç©ºæ–‡å­—
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),    # æ—¥ä»˜(æ—¥)ã‚’å…¥ã‚Œã‚‹
        "month": str(month), # â€»DBã«ã‚«ãƒ©ãƒ ãŒã‚ã‚Œã°å…¥ã‚Œã‚‹ã€ãªã‘ã‚Œã°çœç•¥
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)


# ==================================================
# HTML ãƒ‘ãƒ¼ã‚¹é–¢æ•°ç¾¤
# ==================================================
def parse_race_info(html: str):
    """ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {
        "date_meet": date_meet,
        "race_name": race_name,
        "cond1": cond1,
        "course_line": course_line,
    }

def parse_danwa_comments(html: str):
    """å©èˆã®è©±ï¼ˆè«‡è©±ï¼‰ã‚’ãƒ‘ãƒ¼ã‚¹"""
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}
    danwa_dict = {}
    current = None
    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        if uma_td:
            current = uma_td.get_text(strip=True)
            continue
        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current:
            danwa_dict[current] = danwa_td.get_text(strip=True)
            current = None
    return danwa_dict


def parse_cyokyo(html: str):
    """èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹"""
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    section = None
    h2 = soup.find("h2", string=lambda s: s and "èª¿æ•™" in s)
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None:
        section = soup
    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue
        rows = tbody.find_all("tr", recursive=False)
        if not rows:
            continue
        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")
        if not uma_td or not name_td:
            continue
        umaban = uma_td.get_text(strip=True)
        bamei = name_td.get_text(" ", strip=True)
        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""
        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = ""
        if detail_row:
            detail_text = detail_row.get_text(" ", strip=True)
        final_text = f"ã€é¦¬åã€‘{bamei}ï¼ˆé¦¬ç•ª{umaban}ï¼‰ ã€çŸ­è©•ã€‘{tanpyo} ã€èª¿æ•™è©³ç´°ã€‘{detail_text}"
        cyokyo_dict[umaban] = final_text
    return cyokyo_dict


BASE_URL = "https://s.keibabook.co.jp"
def fetch_cyokyo_dict(driver, race_id: str):
    # åœ°æ–¹ç«¶é¦¬URL: /chihou/cyokyo/1/{ID}
    url = f"{BASE_URL}/chihou/cyokyo/1/{race_id}"
    driver.get(url)
    try:
        # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªãã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ã›ãšã€ç©ºãªã‚‰ç©ºã‚’è¿”ã™
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except Exception:
        return {}
    html = driver.page_source
    return parse_cyokyo(html)


# ==================================================
# â˜…Dify ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–¢æ•°
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot-user",
    }

    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=300, 
        )

        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}\n{res.text}"
            return

        for line in res.iter_lines():
            if line:
                decoded_line = line.decode('utf-8')
                if decoded_line.startswith("data:"):
                    json_str = decoded_line.replace("data: ", "")
                    try:
                        data = json.loads(json_str)
                        event = data.get("event")
                        if event in ["workflow_started", "node_started", "node_finished"]:
                            yield ""
                            continue
                        chunk = data.get("answer", "")
                        if chunk:
                            yield chunk
                        if event == "workflow_finished":
                            outputs = data.get("data", {}).get("outputs", {})
                            if outputs:
                                found_text = ""
                                for key, value in outputs.items():
                                    if isinstance(value, str):
                                        found_text += value + "\n"
                                if found_text:
                                    yield found_text
                                else:
                                    yield f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Raw: {outputs}"
                    except json.JSONDecodeError:
                        pass
                    except Exception as e:
                        yield f"âš ï¸ Parse Error: {str(e)}"

    except Exception as e:
        yield f"âš ï¸ Request Error: {str(e)}"


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†: å…¨ãƒ¬ãƒ¼ã‚¹å®Ÿè¡Œ
# ==================================================
def run_all_races(target_races=None):
    """
    åœ°æ–¹ç«¶é¦¬IDãƒ«ãƒ¼ãƒ«ã«åŸºã¥ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° -> Difyã¸é€ä¿¡ -> ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º -> Supabaseä¿å­˜
    """
    
    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    place_names = {
        "10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ",
        "30": "åœ’ç”°", "42": "é–€åˆ¥", "19": "ç¬ æ¾", "34": "åå¤å±‹",
        "20": "é‡‘æ²¢", "29": "æ°´æ²¢", "33": "ç››å²¡", "58": "å¸¯åºƒ",
        "26": "é«˜çŸ¥", "23": "ä½è³€"
    }
    place_name = place_names.get(PLACE_CODE, "åœ°æ–¹")

    # Selenium è¨­å®š
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    try:
        # --- 1. ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç† ---
        st.info("ğŸ”‘ ç«¶é¦¬ãƒ–ãƒƒã‚¯ã¸ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        driver.get("https://s.keibabook.co.jp/login/login")
        
        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)
        
        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)
        
        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
        ).click()
        
        time.sleep(2)

        st.success("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã€‚ãƒ¬ãƒ¼ã‚¹åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        # --- 2. å„ãƒ¬ãƒ¼ã‚¹å‡¦ç† ---
        for r in race_numbers:
            race_num_str = f"{r:02}"
            
            # â˜…URLç”Ÿæˆãƒ«ãƒ¼ãƒ«å¤‰æ›´ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®š: å¹´ + 11(å›ºå®š) + ç«¶é¦¬å ´ + 01(å›ºå®š) + ãƒ¬ãƒ¼ã‚¹ + æœˆæ—¥ï¼‰
            # ä¾‹: 2025 + 11 + 11(å·å´) + 01 + 01(1R) + 1216(æ—¥ä»˜)
            date_str = f"{MONTH}{DAY}"
            race_id = f"{YEAR}11{PLACE_CODE}01{race_num_str}{date_str}"

            st.markdown(f"### {place_name} {r}R")
            
            status_area = st.empty()
            result_area = st.empty()
            full_answer = ""

            try:
                # ==========================
                # Phase A: ãƒ‡ãƒ¼ã‚¿åé›†ä¸­
                # ==========================
                status_area.info(f"ğŸ“¡ {place_name}{r}R ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­... (ID: {race_id})")
                
                # A-1. å©èˆã‚³ãƒ¡ãƒ³ãƒˆãƒ»åŸºæœ¬æƒ…å ± (åœ°æ–¹URL: /chihou/danwa/1/...)
                url_danwa = f"https://s.keibabook.co.jp/chihou/danwa/1/{race_id}"
                driver.get(url_danwa)
                time.sleep(1)
                html_danwa = driver.page_source
                
                # å–å¾—å¯å¦ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãŒå–ã‚Œã‚‹ã‹ã§åˆ¤æ–­ï¼‰
                race_info = parse_race_info(html_danwa)
                danwa_dict = parse_danwa_comments(html_danwa)

                # A-2. å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã¯å‰Šé™¤ (ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«ã‚ˆã‚Šå‰²æ„›)

                # A-3. èª¿æ•™ (åœ°æ–¹URL: /chihou/cyokyo/1/...)
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

                # A-4. ãƒ‡ãƒ¼ã‚¿çµåˆ
                # åœ°æ–¹ç«¶é¦¬ã¯ã™ã¹ã¦ã®é¦¬ã®ãƒ‡ãƒ¼ã‚¿ãŒæƒã‚ãªã„ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€
                # danwa_dict ã®ã‚­ãƒ¼(é¦¬ç•ª)ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹ã‹ã€1~16ç•ªã¾ã§ãƒ«ãƒ¼ãƒ—ã™ã‚‹ã‹ã€‚
                # ã“ã“ã§ã¯ danwa_dict ã¾ãŸã¯ cyokyo_dict ã«å­˜åœ¨ã™ã‚‹é¦¬ç•ªã‚’ç¶²ç¾…çš„ã«ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã€‚
                all_uma = sorted(list(set(list(danwa_dict.keys()) + list(cyokyo_dict.keys()))), key=lambda x: int(x) if x.isdigit() else 99)

                merged = []
                for uma in all_uma:
                    d_txt = danwa_dict.get(uma, 'ï¼ˆæƒ…å ±ãªã—ï¼‰')
                    c_txt = cyokyo_dict.get(uma, 'ï¼ˆæƒ…å ±ãªã—ï¼‰')
                    
                    text = (
                        f"â–¼[é¦¬ç•ª{uma}]\n"
                        f"  ã€å©èˆã®è©±ã€‘ {d_txt}\n"
                        f"  ã€èª¿æ•™ã€‘ {c_txt}\n"
                    )
                    merged.append(text)

                if not merged:
                    status_area.warning(f"âš ï¸ {place_name} {r}R: ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ(ID: {race_id})ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
                race_header_lines = []
                if race_info["date_meet"]: race_header_lines.append(race_info["date_meet"])
                if race_info["race_name"]: race_header_lines.append(race_info["race_name"])
                if race_info["cond1"]: race_header_lines.append(race_info["cond1"])
                if race_info["course_line"]: race_header_lines.append(race_info["course_line"])
                race_header = "\n".join(race_header_lines)

                merged_text = "\n".join(merged)
                full_text = (
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n"
                    f"{race_header}\n\n"
                    f"ä»¥ä¸‹ã¯{place_name}{r}Rã®ãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚\n"
                    "å„é¦¬ã«ã¤ã„ã¦ã€å©èˆã®è©±ã€‘ãŠã‚ˆã³ã€èª¿æ•™ã€‘ã‚’åŸºã«åˆ†æã›ã‚ˆã€‚\n\n"
                    "â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n"
                    + merged_text
                )

                # ==========================
                # Phase B: AIæ€è€ƒä¸­
                # ==========================
                status_area.info("ğŸ¤– AIãŒåˆ†æãƒ»åŸ·ç­†ä¸­ã§ã™...")
                
                for chunk in stream_dify_workflow(full_text):
                    if chunk:
                        full_answer += chunk
                        result_area.markdown(full_answer + "â–Œ")
                
                # ==========================
                # Phase C: å®Œäº†
                # ==========================
                result_area.markdown(full_answer)
                
                if full_answer:
                    status_area.success("âœ… åˆ†æå®Œäº†")
                    # Supabase ä¿å­˜
                    save_history(
                        YEAR, PLACE_CODE, place_name, MONTH, DAY,
                        race_num_str, race_id, full_answer
                    )
                else:
                    status_area.error("âš ï¸ AIã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã—ãŸã€‚")

            except Exception as e:
                err_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ ({place_name} {r}R): {str(e)}"
                print(err_msg)
                status_area.error(err_msg)
            
            st.write("---")

    finally:
        driver.quit()
