import time
import json
import re
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================

KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¤‰æ•°ï¼ˆapp.pyã‹ã‚‰ä¸Šæ›¸ãã•ã‚Œã‚‹ï¼‰
YEAR = "2025"
PLACE_CODE = "11"
MONTH = "12"
DAY = "16"

def set_race_params(year, place_code, month, day):
    """app.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å·®ã—æ›¿ãˆã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, PLACE_CODE, MONTH, DAY
    YEAR = str(year)
    PLACE_CODE = str(place_code).zfill(2)
    MONTH = str(month).zfill(2)
    DAY = str(day).zfill(2)

# ==================================================
# Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

def save_history(year, place_code, place_name, month, day, race_num_str, race_id, ai_answer):
    supabase = get_supabase_client()
    if supabase is None:
        return
    data = {
        "year": str(year),
        "kai": "", 
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "month": str(month),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }
    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)

# ==================================================
# HTML ãƒ‘ãƒ¼ã‚¹é–¢æ•°ç¾¤
# ==================================================

def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}
    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1: date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2: race_name = ps[1].get_text(strip=True)
    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1: cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2: course_line = sub_ps[1].get_text(" ", strip=True)
    return {"date_meet": date_meet, "race_name": race_name, "cond1": cond1, "course_line": course_line}

def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}
    danwa_dict = {}
    current = None
    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        if uma_td:
            current = uma_td.get_text(strip=True)
            continue
        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current:
            danwa_dict[current] = danwa_td.get_text(strip=True)
            current = None
    return danwa_dict

def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    section = None
    h2 = soup.find("h2", string=lambda s: s and "èª¿æ•™" in s)
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None: section = soup
    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody: continue
        rows = tbody.find_all("tr", recursive=False)
        if not rows: continue
        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")
        if not uma_td or not name_td: continue
        umaban = uma_td.get_text(strip=True)
        bamei = name_td.get_text(" ", strip=True)
        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""
        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = ""
        if detail_row: detail_text = detail_row.get_text(" ", strip=True)
        final_text = f"ã€é¦¬åã€‘{bamei}ï¼ˆé¦¬ç•ª{umaban}ï¼‰ ã€çŸ­è©•ã€‘{tanpyo} ã€èª¿æ•™è©³ç´°ã€‘{detail_text}"
        cyokyo_dict[umaban] = final_text
    return cyokyo_dict

def parse_syutuba_jockey(html: str):
    soup = BeautifulSoup(html, "html.parser")
    jockey_info = {}
    sections = soup.find_all("div", class_="section")
    for sec in sections:
        umaban_div = sec.find("div", class_="umaban")
        if not umaban_div: continue
        umaban = umaban_div.get_text(strip=True)
        kisyu_p = sec.find("p", class_="kisyu")
        if kisyu_p:
            is_change = True if kisyu_p.find("strong") else False
            name = kisyu_p.get_text(strip=True)
            jockey_info[umaban] = {"name": name, "is_change": is_change}
    return jockey_info

# ==================================================
# URL / ID åˆ¶å¾¡ãƒ­ã‚¸ãƒƒã‚¯ (ã“ã“ãŒé‡è¦)
# ==================================================
BASE_URL = "https://s.keibabook.co.jp"

def get_base_race_id(driver, year, month, day, place_name):
    """
    é–‹å‚¬æ—¥ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰ã€ãã®æ—¥ã®ã€Œ1Rã®IDã€ã‚’å–å¾—ã™ã‚‹ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šã€Œé–‹å‚¬å›æ•°ã€ã€Œæ—¥æ•°ã€ã®å¤‰å‹•ã«è‡ªå‹•å¯¾å¿œã™ã‚‹ã€‚
    """
    # é–‹å‚¬æ—¥ãƒšãƒ¼ã‚¸ã¸ã‚¢ã‚¯ã‚»ã‚¹
    date_str = f"{year}{month}{day}"
    url = f"{BASE_URL}/chihou/kaisai_bi/{date_str}"
    
    st.info(f"ğŸ” é–‹å‚¬æƒ…å ±ã‹ã‚‰IDã‚’ç‰¹å®šä¸­... ({url})")
    driver.get(url)
    time.sleep(1)
    
    try:
        # 1. ç«¶é¦¬å ´åã®ãƒªãƒ³ã‚¯ã‚’æ¢ã—ã¦ã‚¯ãƒªãƒƒã‚¯ (ä¾‹: "å·å´")
        # éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã§å¯¾å¿œ
        links = driver.find_elements(By.TAG_NAME, "a")
        target_link = None
        for link in links:
            if place_name in link.text:
                target_link = link
                break
        
        if not target_link:
             st.error(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã«ã€Œ{place_name}ã€ã®é–‹å‚¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥ä»˜ã‹ç«¶é¦¬å ´ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
             return None
        
        target_link.click()
        time.sleep(1)
        
        # 2. ãƒšãƒ¼ã‚¸é·ç§»å¾Œã®URLã¾ãŸã¯ãƒªãƒ³ã‚¯ã‹ã‚‰IDã‚’æ¢ã™
        # å¤šãã®å ´åˆã€ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹1Rã¸é£›ã¶
        
        current_url = driver.current_url
        
        # URLè‡ªä½“ã«ID(16æ¡)ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        match = re.search(r'(\d{16})', current_url)
        if match:
            base_id = match.group(1)
            # ãƒ¬ãƒ¼ã‚¹ç•ªå·éƒ¨åˆ†(10-12æ–‡å­—ç›®)ã‚’01ã«æ­£è¦åŒ–ã—ã¦è¿”ã™
            normalized_id = base_id[:10] + "01" + base_id[12:]
            st.info(f"âœ… IDç‰¹å®šæˆåŠŸ: {normalized_id} (1RåŸºæº–)")
            return normalized_id

        # URLã«ãªã„å ´åˆã€ç”»é¢å†…ã®ã€Œ1Rã€ãªã©ã®ãƒªãƒ³ã‚¯ã‹ã‚‰æ¢ã™
        links = driver.find_elements(By.TAG_NAME, "a")
        for link in links:
            href = link.get_attribute("href")
            if href:
                match = re.search(r'(\d{16})', href)
                if match:
                    base_id = match.group(1)
                    normalized_id = base_id[:10] + "01" + base_id[12:]
                    st.info(f"âœ… IDç‰¹å®šæˆåŠŸ: {normalized_id} (1RåŸºæº–)")
                    return normalized_id
        
        st.error("âš ï¸ ãƒšãƒ¼ã‚¸å†…ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹IDãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
            
    except Exception as e:
        st.error(f"âš ï¸ IDå–å¾—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def fetch_cyokyo_dict(driver, race_id: str):
    # èª¿æ•™URL: /chihou/cyokyo/1/{ID}
    url = f"{BASE_URL}/chihou/cyokyo/1/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo")))
    except: return {}
    return parse_cyokyo(driver.page_source)

def fetch_syutuba_dict(driver, race_id: str):
    # å‡ºé¦¬è¡¨URL: /chihou/syutuba/{ID} (â€»ã“ã“ã«ã¯ /1/ ãŒå…¥ã‚‰ãªã„)
    url = f"{BASE_URL}/chihou/syutuba/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "umaban")))
    except: return {}
    return parse_syutuba_jockey(driver.page_source)

# ==================================================
# Dify ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEYæœªè¨­å®š"
        return
    payload = {"inputs": {"text": full_text}, "response_mode": "streaming", "user": "keiba-bot-user"}
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    try:
        res = requests.post("https://api.dify.ai/v1/workflows/run", headers=headers, json=payload, stream=True, timeout=300)
        if res.status_code != 200:
            yield f"âš ï¸ API Error {res.status_code}"
            return
        for line in res.iter_lines():
            if line:
                decoded = line.decode('utf-8')
                if decoded.startswith("data:"):
                    try:
                        data = json.loads(decoded.replace("data: ", ""))
                        if data.get("event") == "workflow_finished":
                            out = data.get("data", {}).get("outputs", {})
                            yield "".join([v for v in out.values() if isinstance(v, str)])
                        elif "answer" in data:
                            yield data.get("answer", "")
                    except: pass
    except Exception as e:
        yield f"âš ï¸ Req Error: {str(e)}"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==================================================
def run_all_races(target_races=None):
    race_numbers = list(range(1, 13)) if target_races is None else sorted({int(r) for r in target_races})
    place_names = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    place_name = place_names.get(PLACE_CODE, "åœ°æ–¹")

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        driver.get("https://s.keibabook.co.jp/login/login")
        WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
        WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))).send_keys(KEIBA_PASS)
        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))).click()
        time.sleep(2)
        st.success("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")

        # ã€ã“ã“ãŒä¿®æ­£ç‚¹ã€‘ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰å½“æ—¥ã®æ­£ã—ã„IDæ§‹æˆã‚’å–å¾—
        base_id_1r = get_base_race_id(driver, YEAR, MONTH, DAY, place_name)
        
        if not base_id_1r:
            st.error("ğŸ›‘ ãƒ¬ãƒ¼ã‚¹IDãŒç‰¹å®šã§ããªã‹ã£ãŸãŸã‚ä¸­æ–­ã—ã¾ã™ã€‚")
            return

        for r in race_numbers:
            race_num_str = f"{r:02}"
            
            # IDç”Ÿæˆ: å–å¾—ã—ãŸåŸºæº–ID(1R)ã®ã€Œãƒ¬ãƒ¼ã‚¹ç•ªå·éƒ¨åˆ†(10-12æ–‡å­—ç›®)ã€ã ã‘å·®ã—æ›¿ãˆã‚‹
            # ä¾‹: 2025131102 01 1216 -> 2025131102 {r} 1216
            race_id = base_id_1r[:10] + race_num_str + base_id_1r[12:]

            st.markdown(f"### {place_name} {r}R (ID: {race_id})")
            status_area = st.empty()
            result_area = st.empty()
            full_answer = ""

            try:
                status_area.info("ğŸ“¡ ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
                
                # è«‡è©± ( /danwa/1/ID )
                url_danwa = f"https://s.keibabook.co.jp/chihou/danwa/1/{race_id}"
                driver.get(url_danwa)
                time.sleep(1)
                html_danwa = driver.page_source
                race_info = parse_race_info(html_danwa)
                danwa_dict = parse_danwa_comments(html_danwa)

                # å‡ºé¦¬è¡¨ ( /syutuba/ID ) - /1/ç„¡ã—
                syutuba_dict = fetch_syutuba_dict(driver, race_id)

                # èª¿æ•™ ( /cyokyo/1/ID )
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

                all_uma = sorted(list(set(list(danwa_dict.keys()) + list(cyokyo_dict.keys()) + list(syutuba_dict.keys()))), key=lambda x: int(x) if x.isdigit() else 99)
                merged = []
                for uma in all_uma:
                    d = danwa_dict.get(uma, 'ï¼ˆãªã—ï¼‰')
                    c = cyokyo_dict.get(uma, 'ï¼ˆãªã—ï¼‰')
                    j = syutuba_dict.get(uma, {"name": "ä¸æ˜", "is_change": False})
                    alert = "ã€âš ï¸ä¹—ã‚Šæ›¿ã‚ã‚Šã€‘" if j["is_change"] else "ã€ç¶™ç¶šé¨ä¹—ã€‘"
                    merged.append(f"â–¼[é¦¬ç•ª{uma}]\n  ã€é¨æ‰‹ã€‘ {j['name']} {alert}\n  ã€è«‡è©±ã€‘ {d}\n  ã€èª¿æ•™ã€‘ {c}\n")

                if not merged:
                    status_area.warning("ãƒ‡ãƒ¼ã‚¿ãªã—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                prompt = (
                    "â– å½¹å‰²\nå—é–¢æ±ç«¶é¦¬ã®ãƒ—ãƒ­äºˆæƒ³å®¶\n\n"
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n" + "\n".join([v for v in race_info.values() if v]) + "\n\n"
                    "â– æŒ‡ç¤º\nä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å¥¨é¦¬ã‚’åˆ†æã›ã‚ˆã€‚\n"
                    "1. ä¹—ã‚Šæ›¿ã‚ã‚Šã®å½±éŸ¿ã‚’è€ƒå¯Ÿã™ã‚‹ã“ã¨ã€‚\n"
                    "2. é¨æ‰‹ã®ç›¸æ€§ã‚‚è€ƒæ…®ã™ã‚‹ã“ã¨ã€‚(å‚è€ƒ: https://www.nankankeiba.com/leading_kis/180000000003011.do)\n\n"
                    "â– ãƒ‡ãƒ¼ã‚¿\n" + "\n".join(merged)
                )

                status_area.info("ğŸ¤– AIåˆ†æä¸­...")
                for chunk in stream_dify_workflow(prompt):
                    if chunk:
                        full_answer += chunk
                        result_area.markdown(full_answer + "â–Œ")
                
                result_area.markdown(full_answer)
                if full_answer:
                    status_area.success("å®Œäº†")
                    save_history(YEAR, PLACE_CODE, place_name, MONTH, DAY, race_num_str, race_id, full_answer)

            except Exception as e:
                status_area.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            st.write("---")
    finally:
        driver.quit()
