import time
import json
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================

KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ç­‰ã§ set_race_params ãŒå‘¼ã°ã‚Œã‚‹ã¨æ›¸ãæ›ã‚ã‚‹ï¼‰
YEAR = "2025"
KAI = "04"
PLACE = "02"
DAY = "02"


def set_race_params(year, kai, place, day):
    """main.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å·®ã—æ›¿ãˆã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)


# ==================================================
# Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def save_history(
    year: str,
    kai: str,
    place_code: str,
    place_name: str,
    day: str,
    race_num_str: str,
    race_id: str,
    ai_answer: str,
) -> None:
    """history ãƒ†ãƒ¼ãƒ–ãƒ«ã« 1 ãƒ¬ãƒ¼ã‚¹åˆ†ã®äºˆæƒ³ã‚’ä¿å­˜ã™ã‚‹ã€‚"""
    supabase = get_supabase_client()
    if supabase is None:
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)


# ==================================================
# HTML ãƒ‘ãƒ¼ã‚¹é–¢æ•°ç¾¤ (å¤‰æ›´ãªã—)
# ==================================================
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {
        "date_meet": date_meet,
        "race_name": race_name,
        "cond1": cond1,
        "course_line": course_line,
    }


def parse_zenkoso_interview(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼" in s)
    if not h2:
        return []

    midasi = h2.find_parent("div", class_="midasi")
    table = midasi.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return []

    rows = table.tbody.find_all("tr")
    result = []
    i = 0
    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        waku_td = row.find("td", class_="waku")
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")
        if not (waku_td and uma_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = uma_td.get_text(strip=True)
        name = bamei_td.get_text(strip=True)

        prev_date = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        detail = rows[i + 1] if i + 1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                sdata = syoin_td.find("div", class_="syoindata")
                if sdata:
                    ps = sdata.find_all("p")
                    if ps:
                        prev_date = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼":
                        prev_comment = txt

        result.append({
            "waku": waku,
            "umaban": umaban,
            "name": name,
            "prev_date_course": prev_date,
            "prev_class": prev_class,
            "prev_finish": prev_finish,
            "prev_comment": prev_comment,
        })
        i += 2
    return result


def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}
    danwa_dict = {}
    current = None
    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        if uma_td:
            current = uma_td.get_text(strip=True)
            continue
        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current:
            danwa_dict[current] = danwa_td.get_text(strip=True)
            current = None
    return danwa_dict


def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    section = None
    h2 = soup.find("h2", string=lambda s: s and "èª¿æ•™" in s)
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None:
        section = soup
    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue
        rows = tbody.find_all("tr", recursive=False)
        if not rows:
            continue
        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")
        if not uma_td or not name_td:
            continue
        umaban = uma_td.get_text(strip=True)
        bamei = name_td.get_text(" ", strip=True)
        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""
        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = ""
        if detail_row:
            detail_text = detail_row.get_text(" ", strip=True)
        final_text = f"ã€é¦¬åã€‘{bamei}ï¼ˆé¦¬ç•ª{umaban}ï¼‰ ã€çŸ­è©•ã€‘{tanpyo} ã€èª¿æ•™è©³ç´°ã€‘{detail_text}"
        cyokyo_dict[umaban] = final_text
    return cyokyo_dict


BASE_URL = "https://s.keibabook.co.jp"
def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except Exception:
        return {}
    html = driver.page_source
    return parse_cyokyo(html)


# ==================================================
# â˜…Dify ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–¢æ•° (å¼·åŒ–ç‰ˆ)
# ==================================================
def stream_dify_workflow(full_text: str):
    """
    Dify Workflow ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§å‘¼ã³å‡ºã—ã€
    ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å›é¿(è„ˆæ‰“ã¡)ã‚’ã—ã¤ã¤ã€æœ€çµ‚çš„ãªç­”ãˆã‚’ç¢ºå®Ÿã«æŠ½å‡ºã—ã¦yieldã™ã‚‹ã€‚
    """
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿…é ˆ
        "user": "keiba-bot-user",
    }

    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«è¨­å®š(5åˆ†)
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=300, 
        )

        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}\n{res.text}"
            return

        # 1è¡Œãšã¤å—ä¿¡
        for line in res.iter_lines():
            if line:
                decoded_line = line.decode('utf-8')
                if decoded_line.startswith("data:"):
                    json_str = decoded_line.replace("data: ", "")
                    
                    try:
                        data = json.loads(json_str)
                        event = data.get("event")
                        
                        # â˜…é€šä¿¡ç¶­æŒã®ãŸã‚ã®è„ˆæ‰“ã¡ï¼ˆé‡è¦ï¼‰
                        # é€”ä¸­çµŒéã‚¤ãƒ™ãƒ³ãƒˆã®æ™‚ã¯ç©ºæ–‡å­—ã‚’è¿”ã—ã¦ãƒ«ãƒ¼ãƒ—ã‚’æ­¢ã‚ãªã„
                        if event in ["workflow_started", "node_started", "node_finished"]:
                            yield ""
                            continue

                        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: Chatã‚¢ãƒ—ãƒªãƒ©ã‚¤ã‚¯ãªã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
                        chunk = data.get("answer", "")
                        if chunk:
                            yield chunk

                        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: Workflowå®Œäº†æ™‚ã®ä¸€æ‹¬å‡ºåŠ›
                        if event == "workflow_finished":
                            outputs = data.get("data", {}).get("outputs", {})
                            if outputs:
                                # å¤‰æ•°åãŒä½•ã§ã‚ã£ã¦ã‚‚ã€å€¤ãŒã‚ã‚‹ã‚‚ã®ã‚’çµåˆã—ã¦è¿”ã™
                                found_text = ""
                                for key, value in outputs.items():
                                    if isinstance(value, str):
                                        found_text += value + "\n"
                                
                                if found_text:
                                    yield found_text
                                else:
                                    yield f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Raw: {outputs}"

                    except json.JSONDecodeError:
                        pass
                    except Exception as e:
                        yield f"âš ï¸ Parse Error: {str(e)}"

    except Exception as e:
        yield f"âš ï¸ Request Error: {str(e)}"


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†: å…¨ãƒ¬ãƒ¼ã‚¹å®Ÿè¡Œ
# ==================================================
def run_all_races(target_races=None):
    """
    å…¨é ­ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° -> Difyã¸é€ä¿¡ -> ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º -> Supabaseä¿å­˜
    """
    
    # ãƒ¬ãƒ¼ã‚¹ç•ªå·ã®ãƒªã‚¹ãƒˆä½œæˆ
    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_names = {
        "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
        "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
    }
    place_name = place_names.get(PLACE, "ä¸æ˜")

    # Selenium è¨­å®š
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    try:
        # --- 1. ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç† ---
        st.info("ğŸ”‘ ç«¶é¦¬ãƒ–ãƒƒã‚¯ã¸ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        driver.get("https://s.keibabook.co.jp/login/login")
        
        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)
        
        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)
        
        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
        ).click()
        
        time.sleep(2) # é·ç§»å¾…ã¡

        st.success("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã€‚ãƒ¬ãƒ¼ã‚¹åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        # --- 2. å„ãƒ¬ãƒ¼ã‚¹å‡¦ç† ---
        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num

            # UI: ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼
            st.markdown(f"### {place_name} {r}R")
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠï¼ˆã“ã“ã«çŠ¶æ³ã‚’é€æ¬¡å‡ºã™ï¼‰
            status_area = st.empty()
            result_area = st.empty()
            full_answer = ""

            try:
                # ==========================
                # Phase A: ãƒ‡ãƒ¼ã‚¿åé›†ä¸­
                # ==========================
                status_area.info(f"ğŸ“¡ {place_name}{r}R ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­... (æ•°ç§’ã‹ã‹ã‚Šã¾ã™)")
                
                # A-1. å©èˆã‚³ãƒ¡ãƒ³ãƒˆãƒ»åŸºæœ¬æƒ…å ±
                url_danwa = f"https://s.keibabook.co.jp/cyuou/danwa/0/{race_id}"
                driver.get(url_danwa)
                time.sleep(1)
                html_danwa = driver.page_source
                race_info = parse_race_info(html_danwa)
                danwa_dict = parse_danwa_comments(html_danwa)

                # A-2. å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
                url_inter = f"https://s.keibabook.co.jp/cyuou/syoin/{race_id}"
                driver.get(url_inter)
                time.sleep(1)
                zenkoso = parse_zenkoso_interview(driver.page_source)

                # A-3. èª¿æ•™
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

                # A-4. ãƒ‡ãƒ¼ã‚¿çµåˆ
                merged = []
                for h in zenkoso:
                    uma = h["umaban"]
                    text = (
                        f"â–¼[æ {h['waku']} é¦¬ç•ª{uma}] {h['name']}\n"
                        f"  ã€å©èˆã®è©±ã€‘ {danwa_dict.get(uma, 'ï¼ˆç„¡ã—ï¼‰')}\n"
                        f"  ã€å‰èµ°æƒ…å ±ã€‘ {h['prev_date_course']} ({h['prev_class']}) {h['prev_finish']}\n"
                        f"  ã€å‰èµ°è«‡è©±ã€‘ {h['prev_comment'] or 'ï¼ˆç„¡ã—ï¼‰'}\n"
                        f"  ã€èª¿æ•™ã€‘ {cyokyo_dict.get(uma, 'ï¼ˆç„¡ã—ï¼‰')}\n"
                    )
                    merged.append(text)

                if not merged:
                    status_area.warning(f"âš ï¸ {place_name} {r}R: ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
                race_header_lines = []
                if race_info["date_meet"]: race_header_lines.append(race_info["date_meet"])
                if race_info["race_name"]: race_header_lines.append(race_info["race_name"])
                if race_info["cond1"]: race_header_lines.append(race_info["cond1"])
                if race_info["course_line"]: race_header_lines.append(race_info["course_line"])
                race_header = "\n".join(race_header_lines)

                merged_text = "\n".join(merged)
                full_text = (
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n"
                    f"{race_header}\n\n"
                    f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚\n"
                    "å„é¦¬ã«ã¤ã„ã¦ã€å©èˆã®è©±ã€‘ã€å‰èµ°æƒ…å ±ãƒ»å‰èµ°è«‡è©±ã€‘ã€èª¿æ•™ã€‘ã‚’åŸºã«åˆ†æã›ã‚ˆã€‚\n\n"
                    "â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n"
                    + merged_text
                )

                # ==========================
                # Phase B: AIæ€è€ƒä¸­
                # ==========================
                status_area.info("ğŸ¤– AIãŒåˆ†æãƒ»åŸ·ç­†ä¸­ã§ã™... (æ•°åç§’ã€œ1åˆ†ã»ã©ãŠå¾…ã¡ãã ã•ã„)")
                
                # Difyã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—
                for chunk in stream_dify_workflow(full_text):
                    if chunk:
                        full_answer += chunk
                        # æ€è€ƒä¸­ã§ã‚‚ã“ã“ãŒæ›´æ–°ã•ã‚Œã‚‹ï¼ˆã‚«ãƒ¼ã‚½ãƒ«è¡¨ç¤ºï¼‰
                        result_area.markdown(full_answer + "â–Œ")
                
                # ==========================
                # Phase C: å®Œäº†
                # ==========================
                result_area.markdown(full_answer) # æœ€çµ‚çµæœè¡¨ç¤ºï¼ˆã‚«ãƒ¼ã‚½ãƒ«æ¶ˆã™ï¼‰
                
                if full_answer:
                    status_area.success("âœ… åˆ†æå®Œäº†")
                    # Supabase ä¿å­˜
                    save_history(
                        YEAR, KAI, PLACE, place_name, DAY,
                        race_num, race_id, full_answer
                    )
                else:
                    status_area.error("âš ï¸ AIã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã—ãŸã€‚")

            except Exception as e:
                err_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ ({place_name} {r}R): {str(e)}"
                print(err_msg)
                status_area.error(err_msg)
            
            # ãƒ¬ãƒ¼ã‚¹é–“ã®åŒºåˆ‡ã‚Šç·š
            st.write("---")

    finally:
        driver.quit()
