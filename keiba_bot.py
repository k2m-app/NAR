import time
import json
import re
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==========================================
# 1. è¨­å®šãƒ»å®šæ•°
# ==========================================

# ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®å ´æ‰€ã‚³ãƒ¼ãƒ‰(10-13)ã‚’NetKeibaã®URLç”¨ã‚³ãƒ¼ãƒ‰(42-45)ã«å¤‰æ›
# 10:å¤§äº•(44), 11:å·å´(45), 12:èˆ¹æ©‹(43), 13:æµ¦å’Œ(42)
KB_TO_NK_CODE = {
    "10": "44", # å¤§äº•
    "11": "45", # å·å´
    "12": "43", # èˆ¹æ©‹
    "13": "42"  # æµ¦å’Œ
}

# è¡¨ç¤ºç”¨ãƒ»ç…§åˆç”¨ã®å ´æ‰€åãƒãƒƒãƒ—
KB_TO_PLACE_NAME = {
    "10": "å¤§äº•",
    "11": "å·å´",
    "12": "èˆ¹æ©‹",
    "13": "æµ¦å’Œ"
}

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰
st.sidebar.title("è¨­å®š")
YEAR = st.sidebar.text_input("å¹´ (YYYY)", "2025")
MONTH = st.sidebar.text_input("æœˆ (MM)", "12")
DAY = st.sidebar.text_input("æ—¥ (DD)", "26")
PLACE_CODE = st.sidebar.selectbox("é–‹å‚¬å ´æ‰€", ["10", "11", "12", "13"], format_func=lambda x: KB_TO_PLACE_NAME.get(x, x))

# ==========================================
# 2. NetKeibaå°‚ç”¨é–¢æ•°
# ==========================================

def login_netkeiba(driver):
    """
    NetKeibaã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹é–¢æ•°
    secrets.toml ã« [netkeiba] email, password ãŒå¿…è¦
    """
    login_url = "https://regist.netkeiba.com/account/?pid=login"
    try:
        driver.get(login_url)
        time.sleep(1)

        if "logout" in driver.page_source:
            st.info("âœ… NetKeiba: æ—¢ã«ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã§ã™")
            return True

        if "netkeiba" in st.secrets and "email" in st.secrets["netkeiba"]:
            email = st.secrets["netkeiba"]["email"]
            password = st.secrets["netkeiba"]["password"]
            
            driver.find_element(By.NAME, "login_id").send_keys(email)
            driver.find_element(By.NAME, "pswd").send_keys(password)
            
            # ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼ˆã‚¯ãƒ©ã‚¹åãªã©ã¯å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
            submit_btn = driver.find_element(By.CLASS_NAME, "SubmitBtn")
            submit_btn.click()
            time.sleep(2)
            st.success("âœ… NetKeiba: ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")
            return True
        else:
            st.warning("âš ï¸ Secretsã«NetKeibaã®ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æŒ‡æ•°ãŒè¦‹ã‚‰ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            return False
    except Exception as e:
        st.warning(f"âš ï¸ NetKeibaãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—ï¼ˆéãƒ­ã‚°ã‚¤ãƒ³ã§ç¶™ç¶šï¼‰: {e}")
        return False

def get_netkeiba_speed_url(year, month, day, kb_place_code, race_num):
    """
    NetKeibaã‚¿ã‚¤ãƒ æŒ‡æ•°ãƒšãƒ¼ã‚¸ã®URLã‚’ç”Ÿæˆ
    URLä¾‹: https://nar.netkeiba.com/race/speed.html?race_id=202544122601&type=shutuba&mode=past
    """
    nk_place = KB_TO_NK_CODE.get(kb_place_code)
    if not nk_place:
        return None
    
    date_str = f"{month.zfill(2)}{day.zfill(2)}"
    race_str = str(race_num).zfill(2)
    # IDæ§‹æˆ: YYYY + å ´æ‰€ã‚³ãƒ¼ãƒ‰(2æ¡) + MMDD + RR
    race_id = f"{year}{nk_place}{date_str}{race_str}"
    
    return f"https://nar.netkeiba.com/race/speed.html?race_id={race_id}&type=shutuba&mode=past"

def scrape_netkeiba_speed_index(driver, url, current_place_name):
    """
    ã‚¿ã‚¤ãƒ æŒ‡æ•°ãƒšãƒ¼ã‚¸(speed.html)ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    æˆ»ã‚Šå€¤: { "é¦¬ç•ª": { "past_summary": "...", "speed_index": "..." } }
    """
    data = {}
    try:
        driver.get(url)
        time.sleep(1) # èª­ã¿è¾¼ã¿å¾…ã¡
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # --- A. ç¾åœ¨ã®ãƒ¬ãƒ¼ã‚¹è·é›¢ã‚’å–å¾— ---
        current_dist = ""
        race_data_div = soup.find("div", class_="RaceData01")
        if race_data_div:
            text = race_data_div.get_text()
            match = re.search(r'(\d{3,4})m', text)
            if match:
                current_dist = match.group(1)
        
        # ç¾åœ¨ã®æ¡ä»¶æ–‡å­—åˆ—ã‚’ä½œæˆ (ä¾‹: "å¤§äº•ãƒ€1400")
        # åœ°æ–¹ç«¶é¦¬ã¯åŸºæœ¬çš„ã«ãƒ€ãƒ¼ãƒˆå‰æã ãŒã€å¿µã®ãŸã‚
        track_type = "èŠ" if "èŠ" in (race_data_div.get_text() if race_data_div else "") else "ãƒ€"
        current_condition = f"{current_place_name}{track_type}{current_dist}"
        
        st.info(f"ğŸ“ ç¾åœ¨ã®æ¡ä»¶è¨­å®š: {current_condition} (ã“ã‚Œã¨ä¸€è‡´ã™ã‚‹éå»æŒ‡æ•°ã‚’æŠ½å‡ºã—ã¾ã™)")

        # --- B. ãƒ†ãƒ¼ãƒ–ãƒ«è§£æ ---
        table = soup.find("table", class_="SpeedIndex_Table")
        if not table:
            # ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„ã€ã¾ãŸã¯æœ‰æ–™ä¼šå“¡ã§ãªã„å ´åˆãªã©
            st.warning("âš ï¸ ã‚¿ã‚¤ãƒ æŒ‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return {}

        rows = table.find_all("tr", class_="HorseList")
        for row in rows:
            try:
                # 1. é¦¬ç•ªå–å¾—
                umaban_td = row.find("td", class_=re.compile("umaban", re.I))
                if not umaban_td:
                    continue
                umaban = umaban_td.get_text(strip=True)

                # 2. æŒ‡æ•°ã‚«ãƒ©ãƒ ã®ç‰¹å®š
                cols = row.find_all("td")
                # "Horse_Name"ã‚¯ãƒ©ã‚¹ã‚’æŒã¤åˆ—ã‚’æ¢ã—ã€ãã®æ¬¡ã®åˆ—ã‹ã‚‰ãŒæŒ‡æ•°ãƒ‡ãƒ¼ã‚¿
                start_idx = -1
                for i, col in enumerate(cols):
                    if "Horse_Name" in " ".join(col.get("class", [])):
                        start_idx = i + 1
                        break
                
                if start_idx == -1:
                    continue

                # HTMLæ§‹é€ : [é¦¬å] [æœ€é«˜å€¤] [5èµ°å‰] [4èµ°å‰] [3èµ°å‰] [2èµ°å‰] [å‰èµ°]
                # è¿‘5èµ°ã‚’å–å¾—ã—ãŸã„ã®ã§ã€start_idx+1 (5èµ°å‰) ã‹ã‚‰ start_idx+6 (å‰èµ°) ã¾ã§
                # â€»start_idxã¯ã€Œæœ€é«˜å€¤ã€ã®åˆ—
                
                # target_cols = cols[start_idx : start_idx+6] # æœ€é«˜å€¤ã‚‚å«ã‚ã‚‹å ´åˆ
                target_cols = cols[start_idx+1 : start_idx+6] # è¿‘5èµ°ã®ã¿
                
                past_list = []
                speed_match_list = [] # åŒæ¡ä»¶ã®æŒ‡æ•°ãƒªã‚¹ãƒˆ

                for td in target_cols:
                    # <span>å¤§äº•ãƒ€1200</span> H <a>53</a>
                    course_span = td.find("span")
                    if not course_span:
                        continue
                    
                    course_str = course_span.get_text(strip=True) # ä¾‹: "å¤§äº•ãƒ€1200"
                    
                    idx_a = td.find("a")
                    idx_val = idx_a.get_text(strip=True) if idx_a else "-"
                    
                    # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆ "-" ã‚„ç©ºæ–‡å­—
                    if not idx_val.isdigit():
                        continue
                    
                    entry_str = f"{course_str}({idx_val})"
                    past_list.append(entry_str)
                    
                    # â˜…åŒæ¡ä»¶åˆ¤å®šâ˜…
                    if current_condition in course_str:
                         speed_match_list.append(idx_val)
                
                data[umaban] = {
                    "past_summary": " / ".join(past_list) if past_list else "ãªã—",
                    "speed_index": ", ".join(speed_match_list) if speed_match_list else "è©²å½“ãªã—"
                }
                
            except Exception as e:
                continue
                
        return data

    except Exception as e:
        st.error(f"NetKeibaæŒ‡æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


# ==========================================
# 3. ç«¶é¦¬ãƒ–ãƒƒã‚¯ & å…±é€šãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æƒ³å®š)
# ==========================================

def fetch_race_ids_from_schedule(driver, year, month, day, place_code):
    """
    ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ã™ã‚‹ (æ—¢å­˜ã®ã‚‚ã®ã‚’æƒ³å®š)
    """
    # â˜…ã“ã“ã«æ—¢å­˜ã® fetch_race_ids_from_schedule ã®ã‚³ãƒ¼ãƒ‰ã‚’å…¥ã‚Œã¦ãã ã•ã„
    # ãªã‘ã‚Œã°ç°¡æ˜“çš„ãªã‚‚ã®ã‚’è¨˜è¿°ã—ã¾ã™ï¼ˆURLç”Ÿæˆã®ã¿ï¼‰
    # ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®IDå½¢å¼ãŒä¸æ˜ãªãŸã‚ã€æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‚’å„ªå…ˆã—ã¦ãã ã•ã„
    # ä»®å®Ÿè£…:
    date_str = f"{year}{month.zfill(2)}{day.zfill(2)}"
    # æœ¬æ¥ã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦IDãƒªã‚¹ãƒˆã‚’è¿”ã™ã¹ãã§ã™ãŒã€
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç’°å¢ƒã«åˆã‚ã›ã¦ã“ã“ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
    # ä¾‹ã¨ã—ã¦ã€Œ1Rã ã‘ã€è¿”ã™ãƒ€ãƒŸãƒ¼ãƒªã‚¹ãƒˆ
    # return [f"{date_str}{place_code}01"] 
    
    # â†“ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ§˜ã®æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã“ã“ã‚’ç½®ãæ›ãˆã¦ãã ã•ã„ â†“
    st.warning("âš ï¸ `fetch_race_ids_from_schedule` é–¢æ•°ã¯æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
    return [f"20251226{place_code}01"] # ãƒ€ãƒŸãƒ¼ID

def parse_syutuba_jockey(html_source):
    """
    ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®å‡ºé¦¬è¡¨ã‹ã‚‰é¨æ‰‹ãƒ»é¦¬åãªã©ã‚’å–å¾— (æ—¢å­˜ã®ã‚‚ã®ã‚’æƒ³å®š)
    """
    soup = BeautifulSoup(html_source, "html.parser")
    data = {}
    # â˜…ã“ã“ã«æ—¢å­˜ã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’å…¥ã‚Œã¦ãã ã•ã„
    # ä»¥ä¸‹ã¯ãƒ€ãƒŸãƒ¼å®Ÿè£…
    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã—ã¦é¦¬ç•ªã¨é¦¬åã‚’å–å¾—ã™ã‚‹ä¸€èˆ¬çš„ãªå‡¦ç†
        rows = soup.find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) > 5:
                # ç°¡æ˜“çš„ãªåˆ¤å®šï¼ˆå®Ÿéš›ã¯ã‚‚ã£ã¨å³å¯†ã«ï¼‰
                umaban = cols[0].get_text(strip=True)
                horse_name = cols[3].get_text(strip=True)
                if umaban.isdigit():
                    data[umaban] = {"name": horse_name, "is_change": False}
    except:
        pass
    return data

# ==========================================
# 4. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def run_all_races():
    # Chromeã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    # ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã®è¿½åŠ è¨­å®š
    options.add_argument('--disable-gpu')
    options.add_argument("--window-size=1280,1080")
    
    driver = webdriver.Chrome(options=options)

    try:
        st.markdown("## ğŸ‡ ç«¶é¦¬äºˆæƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹")

        # 1. NetKeibaãƒ­ã‚°ã‚¤ãƒ³
        login_netkeiba(driver)

        # 2. ãƒ¬ãƒ¼ã‚¹IDå–å¾— (ç«¶é¦¬ãƒ–ãƒƒã‚¯)
        # â€»æœ¬æ¥ã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å…¨ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—
        # ã“ã“ã§ã¯1Rã€œ12Rã‚’æƒ³å®šã—ã¦ãƒ«ãƒ¼ãƒ—ã€ã‚ã‚‹ã„ã¯æ—¢å­˜é–¢æ•°ã‚’ä½¿ç”¨
        race_ids = fetch_race_ids_from_schedule(driver, YEAR, MONTH, DAY, PLACE_CODE)
        
        # é–‹å‚¬åœ°åã‚’å–å¾—
        current_place_name = KB_TO_PLACE_NAME.get(PLACE_CODE, "ä¸æ˜")

        # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®ãƒ«ãƒ¼ãƒ—
        # â€»race_idsãŒæ­£ã—ãå–å¾—ã§ãã¦ã„ã‚‹å‰æ
        # ã‚‚ã—IDãƒªã‚¹ãƒˆå–å¾—ãŒé›£ã—ã„ãªã‚‰ã€å˜ç´”ã«1~12ã®ãƒ«ãƒ¼ãƒ—ã§URLç”Ÿæˆã—ã¦ã‚‚è‰¯ã„
        
        target_races = [1] # ãƒ†ã‚¹ãƒˆç”¨ã«1Rã®ã¿ã€‚å…¨ãƒ¬ãƒ¼ã‚¹ã‚„ã‚‹ãªã‚‰ range(1, 13)
        
        for race_num in target_races:
            st.markdown(f"### {race_num}R åˆ†æä¸­...")
            
            # --- A. ç«¶é¦¬ãƒ–ãƒƒã‚¯ (èª¿æ•™ãƒ»è«‡è©±) ---
            # URLç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã¯æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã«åˆã‚ã›ã¦ãã ã•ã„
            # ä¾‹: https://s.keibabook.co.jp/chihou/syutuba/202512261001
            kb_race_id = f"{YEAR}{MONTH.zfill(2)}{DAY.zfill(2)}{PLACE_CODE}{str(race_num).zfill(2)}"
            kb_url = f"https://s.keibabook.co.jp/chihou/syutuba/{kb_race_id}"
            
            driver.get(kb_url)
            time.sleep(1)
            kb_source = driver.page_source
            
            # æ—¢å­˜é–¢æ•°ã§ãƒ‘ãƒ¼ã‚¹
            jockey_dict = parse_syutuba_jockey(kb_source)
            # danwa_dict = parse_danwa(kb_source) # æ—¢å­˜ãŒã‚ã‚Œã°
            # cyokyo_dict = parse_cyokyo(kb_source) # æ—¢å­˜ãŒã‚ã‚Œã°
            
            # --- B. NetKeiba (ã‚¿ã‚¤ãƒ æŒ‡æ•°) ---
            nk_url = get_netkeiba_speed_url(YEAR, MONTH, DAY, PLACE_CODE, race_num)
            st.write(f"ğŸ”— NetKeibaå‚ç…§: {nk_url}")
            
            netkeiba_data = {}
            if nk_url:
                netkeiba_data = scrape_netkeiba_speed_index(driver, nk_url, current_place_name)
            
            # --- C. ãƒ‡ãƒ¼ã‚¿çµåˆ ---
            merged_text = []
            
            # é¦¬ç•ªé †ã«ã‚½ãƒ¼ãƒˆ
            all_uma = sorted(list(jockey_dict.keys()), key=lambda x: int(x))
            
            if not all_uma and netkeiba_data:
                 # ç«¶é¦¬ãƒ–ãƒƒã‚¯ã‹ã‚‰é¦¬ç•ªãŒå–ã‚Œãªã‹ã£ãŸå ´åˆã€NetKeibaã®ã‚­ãƒ¼ã‚’ä½¿ã†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                 all_uma = sorted(list(netkeiba_data.keys()), key=lambda x: int(x))

            for uma in all_uma:
                # ç«¶é¦¬ãƒ–ãƒƒã‚¯æƒ…å ±
                j_info = jockey_dict.get(uma, {"name": "åç§°å–å¾—å¤±æ•—"})
                
                # NetKeibaæƒ…å ±
                nk_info = netkeiba_data.get(uma, {})
                past_log = nk_info.get("past_summary", "-")
                speed_idx = nk_info.get("speed_index", "ãªã—")
                
                # çµ¶å¯¾ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ã®å¼·èª¿ãƒ†ã‚­ã‚¹ãƒˆ
                if speed_idx != "ãªã—" and speed_idx != "è©²å½“ãªã—":
                    speed_text = f"â˜…ã€çµ¶å¯¾ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°(åŒæ¡ä»¶)ã€‘: {speed_idx} (ä»Šå›ã®èˆå°ã§å‡ºã—ãŸæŒ‡æ•°)"
                else:
                    speed_text = "  (åŒæ¡ä»¶ã§ã®æŒ‡æ•°è¨˜éŒ²ãªã—)"

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
                line = (
                    f"â–¼[é¦¬ç•ª{uma}] {j_info['name']}\n"
                    # f"  ã€è«‡è©±ã€‘{danwa_dict.get(uma, 'ãªã—')}\n"
                    # f"  ã€èª¿æ•™ã€‘{cyokyo_dict.get(uma, 'ãªã—')}\n"
                    f"  ã€è¿‘5èµ°æŒ‡æ•°å±¥æ­´ã€‘{past_log}\n"
                    f"  {speed_text}\n"
                )
                merged_text.append(line)
            
            final_prompt = "\n".join(merged_text)
            
            # çµæœè¡¨ç¤º
            with st.expander(f"{race_num}R AIå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ç¢ºèª", expanded=True):
                st.text(final_prompt)
            
            # --- D. AIã¸ã®é€ä¿¡ãƒ»ä¿å­˜å‡¦ç† ---
            # ã“ã“ã«æ—¢å­˜ã® Supabase / Gemini / Dify é€£æºã‚³ãƒ¼ãƒ‰ã‚’è¨˜è¿°
            # save_to_supabase(...) 
            # call_ai_api(...)

    finally:
        driver.quit()
        st.success("ğŸ‰ å…¨å·¥ç¨‹å®Œäº†")

# ==========================================
# 5. ã‚¢ãƒ—ãƒªèµ·å‹•
# ==========================================

if st.button("äºˆæƒ³ãƒ‡ãƒ¼ã‚¿ä½œæˆé–‹å§‹"):
    run_all_races()
